#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬ https://rent.thurc.org.taipei/RentHouse/List/5?page=N
è‡ªå‹•ç¿»é  â†’ æŠ“ã€Œåœ°å€ï¼‹ç‹€æ…‹ã€â†’ åªæ¨ã€æ‹›ç§Ÿä¸­ã€â†’ æ¯æ¬¡éƒ½æ›´æ–° seen_addresses.json
"""

import json, os, re, sys, subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Dict

import requests, urllib3
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# ---------- â¶ ç’°å¢ƒè®Šæ•¸ ----------
load_dotenv()
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHAT_ID   = os.getenv("TELEGRAM_CHAT_ID")
assert BOT_TOKEN and CHAT_ID, "æ²’è¨­å®š TELEGRAM_BOT_TOKEN / TELEGRAM_CHAT_IDï¼"

# ---------- â· å¸¸æ•¸ ----------
BASE_URL    = "https://rent.thurc.org.taipei"
LIST_PREFIX = f"{BASE_URL}/RentHouse/List/5?page={{}}"
HEADERS     = {"User-Agent": "Mozilla/5.0 (RentScraper/1.6)"}
STATE_FILE  = Path("seen_addresses.json")

# ---------- â¸ å·¥å…· ----------
def load_seen() -> set[str]:
    try:
        return set(json.loads(STATE_FILE.read_text()))
    except Exception:
        return set()

def save_seen(seen: set[str]) -> None:
    STATE_FILE.write_text(json.dumps(sorted(seen), ensure_ascii=False, indent=2))

def fetch_html(url: str) -> str:
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    r = requests.get(url, headers=HEADERS, timeout=20, verify=False)
    r.raise_for_status()
    return r.text

def get_last_page(soup: BeautifulSoup) -> int:
    last = 1
    for a in soup.select("ul.pagination a.page-link"):
        m = re.search(r"page=(\d+)", a.get("href", ""))
        if m:
            last = max(last, int(m.group(1)))
    return last

def parse_cards(soup: BeautifulSoup) -> List[Dict]:
    listings = []
    for card in soup.select("div.property-item"):
        addr_tag   = card.select_one("span.location")
        # åªæŠ“ç¶ è‰² badge (class=badge-success) æ‰ç®—ã€Œæ‹›ç§Ÿä¸­ã€
        status_tag = card.select_one("span.badge-success")
        if not addr_tag or not status_tag:
            continue

        address = addr_tag.get_text(strip=True)
        status  = status_tag.get_text(strip=True)

        if "æ‹›ç§Ÿ" not in status:        # ä»å†ä¿éšªä¸€æ¬¡
            continue

        title_a = card.select_one("h3.title a, h5.title a")
        url     = BASE_URL + title_a["href"] if title_a and title_a.has_attr("href") else BASE_URL

        listings.append({"address": address, "status": status, "url": url})
    return listings

def send_telegram(text: str) -> None:
    api = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    r = requests.post(api, data={
        "chat_id": CHAT_ID,
        "text": text,
        "disable_web_page_preview": True
    }, timeout=20)
    if r.status_code != 200:
        print("Telegram å¤±æ•—ï¼š", r.text, file=sys.stderr)

# ---------- â¹ ä¸»æµç¨‹ ----------
def main() -> None:
    soup1 = BeautifulSoup(fetch_html(LIST_PREFIX.format(1)), "html.parser")
    last_page = get_last_page(soup1)

    all_listings = parse_cards(soup1)
    for p in range(2, last_page + 1):
        soup = BeautifulSoup(fetch_html(LIST_PREFIX.format(p)), "html.parser")
        all_listings.extend(parse_cards(soup))

    # ç¾å ´æ‰€æœ‰ã€Œæ‹›ç§Ÿä¸­ã€åœ°å€
    snapshot = {it["address"]: it for it in all_listings}

    seen   = load_seen()
    newbies = [snapshot[addr] for addr in snapshot if addr not in seen]

    # === æ¨æ’­ ===
    if newbies:
        send_telegram("\n\n".join(
            f"ğŸ  {it['address']}\nğŸŸ¢ {it['status']}\nğŸ”— {it['url']}"
            for it in newbies
        ))
        print(f"âœ… å·²æ¨é€ {len(newbies)} ç­†æ–°æˆ¿æº")
    else:
        print(datetime.now().strftime("%F %T"), "æ²’æœ‰æ–°æˆ¿æº")

    # === ç„¡è«–æœ‰æ²’æœ‰æ–°äººï¼Œéƒ½æŠŠæœ€æ–° snapshot å¯«å›æª” ===
    save_seen(set(snapshot.keys()))

if __name__ == "__main__":
    main()
